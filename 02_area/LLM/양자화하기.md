양자화하기위해서는 다수의 패키지가 필요하다.
- `bitsandbytes`: CUDA 사용자 정의 함수의 경량 래퍼로 특히 8비트 최적화 프로그램, 행렬 곱셈 및 양자화 함수에 대한 래퍼이다.
- PEFT(Parameter-Efficient Fine Tuning): 모델의 모든 매개변수를 미세 조정하지 않고도 사전 훈련된 언어모델을 다양한 다운스트림 애플리케이션에 적용할 수 있도록 해준다.
- `accelerate`: PyTorch 모델을 여러 컴퓨터나 GPU에서 더욱 쉽게 사용할 수 있게해주는 도구
```bash
pip install -qU bitsandbytes
pip install -qU git+https://github.com/huggingface/transformers.git
pip install -qU git+https://github.com/huggingface/perft.git
pip install -qU git+https://github.com/huggingface/accelerate.git
```

# `BitsandBytesConfig`  양자화 매개변수 정의하기
- `load_in_4Bit=True`: 모델을 4비트 정밀도로 변환하고 로드하도록 지정
- `bnb_4bit_use_double_quant=True`: 메모리 효율을 높이기 위해 중첩 양자화를 사용하여 추론과 학습진행
- `bnt_4bit_quant_type="nf4"`: 4비트 통합에 재공되는 FP4와 NF4 두가지 양자화 타입 중 Normal Float 4를 나타내는 NF4 사용.
- `bnb_4bit_compute_dtype=torch.bfloat16`: 계산 중 사용할 dtype을 변경할 때 사용할 타입. 기본적으로 계산 dtype은 `float32` 이지만 계산 속도를 높이기 위해 `bfloat16`으로 사용
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesonfig

bnb_config = BitsAndBytesConfig(
	load_in_4bit=True,
	bnb_4bit_use_double_quant=True,
	bnb_4bit_quant_type="nf4",
	bnb_4bit_compute_dtype=torch.bfloat16
)
```

# 경량화 모델 로드하기
모델 ID를 지정하고, 위에 정의한 양자화 구성으로 로드한다.

```python
model_id = "kyujinpy/Ko-PlatYi-6B"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")
```
이와 같이 하면 모델의 `nn.Linear` 레이어가 `bnb.nn.Linear4` 레이어로 대체된다.

# 실제 테스트
```python

message = [
	{
		'role': 'user',
		'content': '은행의 기준 금리에 대해 설명해줘.'
	}
]

encoded = tokenizer.apply_chat_template(messages, return_tensors = "pt")
model_inputs = encoded.to(device)

generated_ids = model.generate(model_inputs, max_new_tokens = 1000, do_sample = True)
decoded = tokenizer.batch_decode(generated_ids)

```

# RAG 진행

## 의존성 모듈 구성

utf-8 , ansi 관련 오류의 경우를 대비하여 다음과 같이 코드를 추가한다.(CoLab 이나 jupter노트북인 경우)

```python
import locale

def getpreferredencoding(do_setlocate=True):
	return "UTF-8"
locale.getpreferredencoding = getpreferredencoding
```

`langchain` 등 필요한 의존성을 추가한다.

```bash
pip -q install langchain pypdf chromadb sentence-transformers faiss-gpu
```

```python
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from transformers import pipeline
from langchain.chains import LLMChain

text_generation_pipeline = pipeline(
	model=model,
	tokenizer=tokenizer,
	task="tet-generation",
	temperature=0.2,
	return_full_text=True,
	max_new_tokens=300,
)

prompt_template="""
### [INST]
Instruction: Answer the question based on your knowledge.
Here is context to help:

{context}

### QUESTION:
{question}

[/INST]
"""

koplaytiy_llm = HuggingFacePipeline(pipeline = text_generation_pipeline)

# Prompt 생성
prompt = PromptTemplate( 
	input_variables=["context", "question"],
	template=prompt_template	
)

# llm chain0
llm_chain = LLMChain(llm=koplaytiy_llm, prompt=prompt)

### RAG
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.document_loaders import PyPDFLoader
from langchain.schema.runnable import RunnablePassthrough

loader = PyPDFLoader("....pdf")
pages = loader.load_and_split()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_document(pages)

from langchain.embeddings import HuggingFaceEmbeddings

model_name="jhgan/ko-sbert-nli"
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceEmbeddings(
	model_name=model_name,
	encode_kwargs=encode_kwargs
)

db = FAISS.from_documents(texts, hf)
retriever = db.as_retriever(
	search_type="similarity",
	search_kwargs={'k': 3}
)

# context를 넣어줄 때 retriever로 결정하고, question 은 외부에서 오는 것 그대로 넣는다.
# 지정된 prompt는 llm_chain을 타고 모델을 호출하게 된다. 
rag_chain = (
	{"context": retriever, "question": RunnablePassthrough()}
	| llm_chain
)

# warning 무시하기
import warnings
warnings.filterwarnings('ignore')

result = rag_chain.invoke("혁신성장 정책 금융에서 인공지능의 가능성은?")

for i in result['context']:
	print(f"근거: {i.page_content} / 출처: {i.metadata['source']} - {i.metadata['page']} \n\n")

print(f"답변: {result['text']}")
```