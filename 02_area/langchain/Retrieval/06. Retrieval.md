사용자의 질문을 임베딩 모델을 통해 임베딩한 후 벡터 저장소에 들어있는 벡터와 비교하여 이를 유사문장을 생성하고, LLM에 해당 변수를 Chain을 통해 전달하는 부분이다.

# 사전지식 - Chain

## Stuff
분할된 텍스트 청크를 Context에 그대로 주입한다. 이 때 토큰 크기 관련 이슈가 발생할 수 있어, 주의가 필요하다.
질문과 Context를 전달하면, Prompt 에 그대로 해당 질문과 Context를 대치하여 전달한다.
![[rag_chain_stuff.png]]


## Map_reduce
분할된 텍스트 청크마다 요약을 생성하고, 이를 합친 최종 요약을 생성한다. LLM 모델 호출이 다수 발생하여 느린 단점이 있다. Map과정에서는 텍스트 문단마다의 요약을 만들며, Reduce 과정에서 이 요약을 모아 새로이 요약한다. Stuff 에서 발생하는 Token 이슈를 피할 수 있다.

![[rag_chain_map_reduce.png]]


## Refine
분할된 텍스트 청크를 순회하면서 누적 답변을 생성한다. 품질이 뛰어나지만, Map_reduce와 마찬가지로 LLM 모델을 반복적으로 호출하기때문에 시간이 오래 걸린다.
![[rag_chain_refine.png]]

## Map_rerank
텍스트 청크를 순회하면서 답변과 점수를 생성한다. 생성한 답변 중 가장 점수가 높은 항목을 채택하는 방식. 비용이 많이 발생할 수 있지만, 텍스트 청크가 서로 다른 내용이며, 답변의 품질이 중요할 때 유용하다.
![[rag_chain_map_rerank.png]]

## 코드
Retriever는 쿼리가 주어질 때 문서를 반환하는 인터페이스이다. 문서를 저장하지않고, 반환(검색)만 할 수 있다.

```bash
pip install chromadb tiktoken transformers sentence_transformers openai langchain pypdf
```

```python
import tiktoken

tokenizer = tiktoken.get_encoding("cl100k_base")

def tiktoken_len(text):
  tokens = tokenizer.encode(text)
  return len(tokens)

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.document_loaders import PyPDFLoader

# 문서 읽기 
loader = PyPDFLoader("...pdf")
pages = loader.load_and_split()

# Splitter로 문서를 나는다.
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50, length_function=tiktoken_len)
texts = text_splitter.split_documents(pages)

# Embedding 처리
from langchain.embeddings import HuggingFaceEmbeddings
model_name = "jhgan/ko-sbert-nli"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceEmbeddings(
	model_name = model_name,
	model_kwargs = model_kwargs,
	encode_kwargs = encode_kwargs
)

# Vector Store로 문서를 hf모델을 통해 embedding 한 후 이를 Chroma Vector Store에 저장
docsearch = Chroma.from_documents(texts, hf)

# 답변을 스트림으로 추력
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

openai = ChatOpenAI(model_name="gpt-3.5-turbo", streaming=True, callbacks=[StreamingStdOutCallbackhandler()], temperature = 0)

# Retrieval 수행
qa = RetrievalQA.from_chain_type(llm = openai,
	chain_type='stuff',
	retriever = docsearch.as_retriever(
	  search_type='mmr',   # Vector 저장소에서 연관 Chunk를 뽑는 방식 - mmr: 연관성 높은 풀 일정 갯수 안에서 최대한 다양성을 가진채로 가져오기
	  search_kwargs={'k':3, 'fetch_k': 10}), # mmr의 파라미터 : fetch_k 는 연관문서 후보의 갯수, k 는 최종적으로 연관성을 통해 가져오게될 청크의 갯수. 총 10개의 연관성있는 문서를 뽑아 3개만 가져오기.
	return_source_documents = True)
query = "질문..."
result = qa(query)
